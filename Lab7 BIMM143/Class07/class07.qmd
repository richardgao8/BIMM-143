---
title: "class07: Machine Learning 1"
author: "Richard Gao (PID: A16490010)"
format: pdf
---

# Clustering Methods

The broad goal here is to find grouping (clusters) in your input data.

## Kmeans

First, let's make up some data to cluster.


```{r}
x <- rnorm(1000)
hist(x)
```

Make a vector of length 60 with 30 points centered at -3 and 30 points centered at +3
```{r}
tmp <- c( rnorm(30, mean=-3), rnorm(30, mean=3) )
```

I will now make a wee x and y dataset with 2 groups of points.


```{r}
x <- cbind(x=tmp, y=rev(tmp))
plot(x)
```

```{r}
k <- kmeans(x, centers=2)
k
```

> Q. From your result object `k` how many points are in each cluster?

```{r}
k$size
```

> Q. What "component" of your result object details the cluster membership?

```{r}
k$cluster
```


> Q. Cluster centers?

```{r}
k$centers
```

> Q Plot of our cluster results?

```{r}
plot(x, col=k$cluster)
points(k$centers, col="blue", pch=15, cex=2)
```

We can cluster into 4 groups
```{r}
k_4 <- kmeans(x, centers=4)
k_4
```

Plot of Results with 4 Centers
```{r}
plot(x, col=k_4$cluster)
points(k_4$centers, col="green", pch=15, cex=2)
```
A big limitation of kmeans is that it does what you ask even if you ask for silly clusters

## Hierarchical Clustering

The main base R function for HIerarchical Clustering is `hclust()`. Unlike `kmeans()` you can not just pass it your data as input. You first need to calculate a distance matrix.

```{r}
d <- dist(x)
hc <- hclust(d)
hc
```

Use `plot()` to view results

```{r}
plot(hc)
abline(h=10, col="red")
```

To make the "cut" and get our cluster membership vector we can use the `cutree()` function.

```{r}
grps <- cutree(hc, h=10)
grps
```

Make a plot of our data colored by hclust results

```{r}
plot(x, col=grps)
```

# Principal Component ANalysis (PCA)

Here we will do Principal Component Analysis (PCA) on some food data from the UK.

```{r}
url <- "https://tinyurl.com/UK-foods"
x <- read.csv(url, row.names=1) # sets the first column to be "row numbers" (actually names)
View(x)
```

Q1. How many rows and columns are in your new data frame named x? What R functions could you use to answer this questions?
```{r}
dim(x)
```

## Preview the first 6 rows
```{r}
head(x)
```

Note how the minus indexing works - Set the first column to be row numbers.
```{r}
#rownames(x) <- x[ ,1]
#x <- x[, 1]
#head(x)
```

Q2. Which approach to solving the ‘row-names problem’ mentioned above do you prefer and why? Is one approach more robust than another under certain circumstances?
I prefer the editting of the read.csv function since the other method is iterative and will begin deleting columns when it's run repeatedly.
```{r}
barplot(as.matrix(x), beside=T, col=rainbow(nrow(x)))
```
Q3: Changing what optional argument in the above barplot() function results in the following plot?
```{r}
barplot(as.matrix(x), beside=F, col=rainbow(nrow(x)))
```

Q5: Generating all pairwise plots may help somewhat. Can you make sense of the following code and resulting figure? What does it mean if a given point lies on the diagonal for a given plot?
- Each individual box is the food information of one country plotted against another country. If a given point lies on the diagonal for a given plot it means that each country has the same value.
```{r}
pairs(x, col=rainbow(10), pch=16)
```

## PCA to the rescue

The main "base" R function for PCA is called `prcomp()`. Here we need to take the transpose of our input as we want the countries in the rows and food as the columns.

Q6. What is the main differences between N. Ireland and the other countries of the UK in terms of this data-set?
N. Ireland is more different from the other countries of the UK in the consumption of foods, since the graphs involving N. Ireland deviate more from the diagonal than the graphs not involving them. Particularly, the blue and orange points are most different from that of other countries but we are unable to identify what foods they correspond to.

```{r}
pca <- prcomp(t(x))
summary(pca)
```

> Q. How much variance is captured in 2 PCs

96.5%

To make our main "PC score plot" or "PC1 vs PC2 plot", or "PC plot" or "ordienation plot".

```{r}
attributes(pca)
```

We are after the `pca$x` result component to make our main PCA plot.

```{r}
pca$x
```

Q7. Complete the code below to generate a plot of PC1 vs PC2. The second line adds text labels over the data points.
```{r}
mycols <- c("orange", "red", "blue", "darkgreen")
plot(pca$x[,1], pca$x[,2], col=mycols, pch=16, xlab="PC1 (67%)", ylab="PC2 (29%", xlim=c(-270, 500))
text(pca$x[,1], pca$x[,2], colnames(x))
```

Q8. Customize your plot so that the colors of the country names match the colors in our UK and Ireland map and table at start of this document.
```{r}
mycols <- c("orange", "red", "blue", "darkgreen")
plot(pca$x[,1], pca$x[,2], col=mycols, pch=16, xlab="PC1 (67%)", ylab="PC2 (29%", xlim=c(-270, 500))
text(pca$x[,1], pca$x[,2], colnames(x), col = mycols)
```

Another important result from PCA is how the original variables (in this case the foods) contribute to the PCs.

This is contained in the `pca$rotation` object - folks often call this the "loadings" or "contributions" to the PCs

```{r}
pca$rotation
```

We can make a plot along PC1.

```{r}
library(ggplot2)

contrib <- as.data.frame(pca$rotation)

ggplot(contrib) +
  aes(PC1, rownames(contrib)) +
  geom_col()
```

Q9: Generate a similar ‘loadings plot’ for PC2. What two food groups feature prominantely and what does PC2 maninly tell us about?
- Fresh potatoes and soft drinks.
- PC2 mainly tells us that the secondary variance is driven mainly by the difference in production of fresh potatoes and soft drinks.
```{r}
ggplot(contrib) +
  aes(PC2, rownames(contrib)) +
  geom_col()
```

